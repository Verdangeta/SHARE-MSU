{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ajoVjdRCgPr"
   },
   "source": [
    "# Практическое задание 2\n",
    "\n",
    "## Данные о студенте\n",
    "\n",
    "1. **ФИО**: Мироненко Александр Витальевич\n",
    "2. **Факультет**: Мехмат\n",
    "3. **Курс**: 4\n",
    "4. **Группа**: 408\n",
    "\n",
    "## Замечания\n",
    "\n",
    "* Название ноутбука с реализацией должно иметь шаблон \"**Prac02_Ivanov.ipynb**\" и посылаться на почту mlcoursemm@gmail.com с темой **[CV2021:Prac02]**\n",
    "* Дедлайн будет оговорен в чате курса\n",
    "* Соблюдаем кодекс чести (по нулям и списавшему, и давшему списать)\n",
    "* Можно (и нужно!) применять для реализации только библиотеку **Numpy**\n",
    "* Ничего, крому Numpy, нельзя использовать для реализации \n",
    "* **Keras** используется только для тестирования Вашей реализации\n",
    "* Если какой-то из классов не проходит приведенные тесты, то соответствующее задание не оценивается\n",
    "* Возможно использование дополнительных (приватных) тестов\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0zJnVx2CgPs"
   },
   "source": [
    "## Реализация собственного нейросетевого пакета для запуска и обучения нейронных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eILKzbGCgPt"
   },
   "source": [
    "Задание состоит из трёх частей:\n",
    "1. Реализация прямого вывода нейронной сети (первое практическое задание)\n",
    "2. Реализация градиентов по входу и распространения градиента по сети (back propagation)\n",
    "3. Реализация градиентов по параметрам и метода обратного распространения ошибки с обновлением парметров сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6at1lHy9CgPt"
   },
   "source": [
    "###  1. Реализация вывода собственной нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8fEuGOSCgPu"
   },
   "source": [
    "1.1 Внимательно ознакомьтесь с интерфейсом слоя. Любой слой должен содержать как минимум три метода:\n",
    "- конструктор\n",
    "- прямой вывод \n",
    "- обратный вывод, производные по входу и по параметрам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEGOpC0QCgPu"
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.name = 'Layer'       \n",
    "    def forward(self, input_data):\n",
    "        pass\n",
    "    def backward(self, input_data):\n",
    "        return [self.grad_x(input_data), self.grad_param(input_data)]\n",
    "    \n",
    "    def grad_x(self, input_data):\n",
    "        pass\n",
    "    def grad_param(self, input_data):\n",
    "        return []\n",
    "    \n",
    "    def update_param(self, grads, learning_rate):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weXCwz-fCgPu"
   },
   "source": [
    "1.2 Ниже предствален интерфейс класса  Network. Обратите внимание на реализацию метода predict, который последовательно обрабатывает входные данные слой за слоем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmFJgJB4CgPv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, layers, loss=None):\n",
    "        self.name = 'Network'\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        return self.predict(input_data)\n",
    "    \n",
    "    def grad_x(self, input_data, labels):\n",
    "        current_input = input_data\n",
    "        grad = self.layers[0].grad_x(current_input)\n",
    "\n",
    "        current_input = self.layers[0].forward(current_input)\n",
    "        for i in range(1,len(self.layers)):\n",
    "\n",
    "            current_grad = self.layers[i].grad_x(current_input)  \n",
    "            current_input = self.layers[i].forward(current_input) \n",
    "\n",
    "            grad_new = np.zeros((input_data.shape[0],current_grad.shape[1],grad.shape[2]))\n",
    "\n",
    "            for item in range(input_data.shape[0]):\n",
    "              grad_new[item] = current_grad[item].dot(grad[item])\n",
    "            \n",
    "            grad = grad_new\n",
    "        \n",
    "        current_grad = self.loss.grad_x(current_input,labels)\n",
    "        current_grad = current_grad.reshape(input_data.shape[0],1,current_grad.shape[1])\n",
    "        grad_new = np.zeros((input_data.shape[0],current_grad.shape[1],grad.shape[2]))\n",
    "\n",
    "        for item in range(input_data.shape[0]):\n",
    "              grad_new[item] = current_grad[item].dot(grad[item])\n",
    "        grad = grad_new\n",
    "\n",
    "        return grad.reshape(input_data.shape)\n",
    "    \n",
    "    def grad_param(self, input_data, labels):\n",
    "        grad_param = []\n",
    "        output = [input_data]\n",
    "\n",
    "        current_input = input_data\n",
    "        for layer in self.layers:\n",
    "            current_input = layer.forward(current_input)   \n",
    "            output.append(current_input) \n",
    "\n",
    "        output_last = self.forward(input_data)##можно упростить\n",
    "        dx = self.loss.grad_x(output_last, labels).reshape((input_data.shape[0],1,-1))\n",
    "\n",
    "        for layer in range(len(self.layers)-1,-1,-1):\n",
    "\n",
    "          dxdw = self.layers[layer].grad_param(output[layer])\n",
    "          if dxdw == []:\n",
    "            grad_param.append(dxdw)\n",
    "          else:\n",
    "            \n",
    "            prom = [np.empty((input_data.shape[0],dx.shape[1],dxdw[0].shape[-1])),np.empty((input_data.shape[0],dxdw[1].shape[-1]))]\n",
    "            for i in range(input_data.shape[0]):\n",
    "              for j in range(len(dxdw)):\n",
    "                prom[j][i] = dx[i].dot(dxdw[j][i])\n",
    "            dxdw = prom\n",
    "                                     \n",
    "            grad_param.append(dxdw)\n",
    "            \n",
    "          arr = self.layers[layer].grad_x(output[layer])\n",
    "          dx_new = np.empty((input_data.shape[0],dx.shape[1],arr.shape[-1]))\n",
    "          for i in range(input_data.shape[0]):\n",
    "            dx_new[i] = dx[i].dot(arr[i])\n",
    "          dx = dx_new\n",
    "        grad_param.reverse()\n",
    "        return grad_param\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update(self, grad_list, learning_rate):\n",
    "      for i in range(len(self.layers)):\n",
    "        self.layers[i].update_param(grad_list[i],learning_rate)\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        current_input = input_data\n",
    "        for layer in self.layers:\n",
    "            current_input = layer.forward(current_input)     \n",
    "        return current_input\n",
    "    \n",
    "    def calculate_loss(self, input_data, labels):\n",
    "        return self.loss.forward(self.predict(input_data), labels)\n",
    "    \n",
    "    def train_step(self, input_data, labels, learning_rate=0.001):\n",
    "        grad_list = self.grad_param(input_data, labels)\n",
    "        self.update(grad_list, learning_rate)\n",
    "    \n",
    "    \n",
    "    def fit(self, trainX, trainY, validation_split=0.25, \n",
    "            batch_size=1, nb_epoch=1, learning_rate=0.01):\n",
    "        \n",
    "        train_x, val_x, train_y, val_y = train_test_split(trainX, trainY, \n",
    "                                                          test_size=validation_split,\n",
    "                                                          random_state=42)\n",
    "        for epoch in range(nb_epoch):\n",
    "            #train one epoch\n",
    "            for i in tqdm(range(int(len(train_x)/batch_size))):\n",
    "                batch_x = train_x[i*batch_size: (i+1)*batch_size]\n",
    "                batch_y = train_y[i*batch_size: (i+1)*batch_size]\n",
    "                self.train_step(batch_x, batch_y, learning_rate)\n",
    "            #validate\n",
    "            val_accuracy = self.evaluate(val_x, val_y)\n",
    "            print('%d epoch: val %.2f' %(epoch+1, val_accuracy))\n",
    "            \n",
    "    def evaluate(self, testX, testY):\n",
    "        y_pred = np.argmax(self.predict(testX), axis=1)            \n",
    "        y_true = np.argmax(testY, axis=1)\n",
    "        val_accuracy = np.sum((y_pred == y_true))/(len(y_true))\n",
    "        return val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2NruOYDCgPv"
   },
   "source": [
    "#### 1.1 (6 баллов) Необходимо реализовать метод forward для вычисления следующих слоёв:\n",
    "\n",
    "- DenseLayer\n",
    "- ReLU\n",
    "- Softmax\n",
    "- FlattenLayer\n",
    "- MaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mSxtHocCgPv"
   },
   "outputs": [],
   "source": [
    "#импорты\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2MSrtFmCgPw"
   },
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    def __init__(self, input_dim, output_dim, W_init=None, b_init=None):\n",
    "        self.name = 'Dense'\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        if W_init is None or b_init is None:\n",
    "            self.W = np.random.random((input_dim, output_dim))\n",
    "            self.b = np.zeros(output_dim, 'float32')\n",
    "        else:\n",
    "            self.W = W_init\n",
    "            self.b = b_init\n",
    "    def forward(self, input_data):\n",
    "      \n",
    "      assert input_data.shape[1] == self.input_dim, \"alert!!! different shapes\"\n",
    "\n",
    "      out = input_data.dot(self.W) + self.b\n",
    "      return out\n",
    "      \n",
    "    def grad_x(self, input_data):\n",
    "        out = np.zeros((input_data.shape[0],self.W.shape[1],self.W.shape[0]))\n",
    "        for i in range (input_data.shape[0]):\n",
    "          out[i] = self.W.T\n",
    "        return out\n",
    "    def grad_b(self, input_data):\n",
    "        out = np.zeros((input_data.shape[0],self.W.shape[1],self.W.shape[1]))\n",
    "        for i in range(input_data.shape[0]):\n",
    "          out[i] = np.eye(self.W.shape[1])\n",
    "        return out    \n",
    "\n",
    "    def grad_W(self, input_data):\n",
    "      out =  np.zeros((input_data.shape[0],self.W.shape[1],self.W.shape[0],self.W.shape[1]))\n",
    "      for item in range(input_data.shape[0]):\n",
    "        for i in range(self.W.shape[1]):\n",
    "          out[item,i,:,i] = input_data[item]\n",
    "          \n",
    "      return out.reshape(input_data.shape[0],self.W.shape[1],-1)\n",
    "    \n",
    "    def update_W(self, grad, learning_rate):\n",
    "        self.W -= learning_rate * np.mean(grad, axis=0).reshape(self.W.shape)\n",
    "    \n",
    "    def update_b(self, grad,  learning_rate):\n",
    "      self.b -= learning_rate * np.mean(grad, axis=0)\n",
    "        \n",
    "    def update_param(self, params_grad, learning_rate):\n",
    "        self.update_W(params_grad[0], learning_rate)\n",
    "        self.update_b(params_grad[1], learning_rate)\n",
    "    \n",
    "    def grad_param(self, input_data):\n",
    "        return [self.grad_W(input_data), self.grad_b(input_data)]\n",
    "    \n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = 'ReLU'\n",
    "    def forward(self, input_data):\n",
    "      \n",
    "\n",
    "      relu = lambda x: np.maximum(0,x)\n",
    "      out = np.array(list(map(relu,input_data)))\n",
    "      \n",
    "      return out\n",
    "\n",
    "    def grad_x(self, input_data):\n",
    "      out = np.zeros((input_data.shape[0],input_data.shape[1],input_data.shape[1]))\n",
    "      for i in range(input_data.shape[0]) :\n",
    "        diag = np.diag(input_data[i])\n",
    "        out[i] = diag\n",
    "      out[out>0] = 1\n",
    "      out[out<= 0] = 0\n",
    "      return out\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class Softmax(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = 'Softmax'\n",
    "    def forward(self, input_data):\n",
    "\n",
    "      out = np.exp(input_data - np.max(input_data,axis = 1)[:,np.newaxis]) \n",
    "\n",
    "      out = np.array([i/np.sum(i) for i in out])\n",
    "      \n",
    "      return out\n",
    "\n",
    "    def grad_x(self, input_data):\n",
    "      reteiner = np.zeros((input_data.shape[0],input_data.shape[1],input_data.shape[1]))\n",
    "\n",
    "      for i in range(input_data.shape[0]):\n",
    "\n",
    "        max_out = np.max(input_data[i])\n",
    "        exp_out = np.exp(input_data[i]-max_out).reshape((1,input_data.shape[1]))\n",
    "        \n",
    "        diag_exp = np.diag(np.exp(input_data[i]-max_out)/np.sum(exp_out))\n",
    "\n",
    "        out = -exp_out.T.dot(exp_out)/(np.sum(exp_out))**2\n",
    "        out = out + diag_exp\n",
    "        reteiner[i] = out\n",
    "\n",
    "      return  reteiner\n",
    "    \n",
    "\n",
    "\n",
    "class FlattenLayer(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = 'Flatten'\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        out = np.empty((input_data.shape[0],input_data[0].size))\n",
    "        \n",
    "        out[0] = input_data[0].flatten().reshape(1,input_data[0].size)\n",
    "        \n",
    "        for i in range(1,input_data.shape[0]):\n",
    "            out[i] = input_data[i].flatten().reshape(1,input_data[0].size)  \n",
    "        return out\n",
    "        \n",
    "    def grad_x(self):\n",
    "        pass\n",
    "\n",
    "class MaxPooling(Layer):\n",
    "    def __init__(self, pool_size=(2, 2), strides=2):\n",
    "        self.name = 'MaxPooling'\n",
    "        self.pool_size = pool_size\n",
    "        self.strides = 2\n",
    "    def forward(self, input_data):\n",
    "      # На входе - четырехмерный тензор вида [batch, input_channels, height, width]\n",
    "      # Нужно заполнить Numpy-тензор out \n",
    "        out = np.empty((input_data.shape[0], input_data.shape[1], 1+(input_data.shape[2]-self.pool_size[0])// self.strides,1+ (input_data.shape[3]-self.pool_size[1])// self.strides))\n",
    "        \n",
    "        for i in range(input_data.shape[0]):\n",
    "            for j in range(input_data.shape[1]):\n",
    "                for k in range(0,input_data.shape[2],self.strides):\n",
    "                    for e in range(0,input_data.shape[3],self.strides):\n",
    "                        \n",
    "                        if k+self.pool_size[0] >input_data.shape[2] or e+self.pool_size[1]> input_data.shape[3]:\n",
    "                            continue\n",
    "                        out[i,j,k // self.strides,e // self.strides] = input_data[i, j, k:(k+self.pool_size[0]), e:(e+self.pool_size[1])].max()\n",
    "        return out\n",
    "    def grad_x(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQPDSwuuCgPw"
   },
   "source": [
    "#### 1.2 (3 балла) Реализуйте теперь свёртночный слой   (опционально)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43x-7xzgCgPx"
   },
   "outputs": [],
   "source": [
    "class Conv2D(Layer):\n",
    "    def __init__(self, kernel_size, input_channels, output_channels, \n",
    "                 kernels_init=None, bias_init=None):\n",
    "        self.name = 'Conv2D'\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_channels = input_channels\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        if kernels_init is None or bias_init is None:\n",
    "            pass\n",
    "        else:\n",
    "            self.kernels = kernels_init\n",
    "            self.bias = bias_init\n",
    "            \n",
    "    def forward(self, input_data):\n",
    "        pass\n",
    "    def grad_x(self):\n",
    "        pass\n",
    "    def grad_kernel(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1U3k-OwHCgPx"
   },
   "source": [
    "#### 1.4 Теперь настало время теста. \n",
    "#### Если вы всё сделали правильно, то запустив следующие ячейки у вас должна появиться надпись: Test PASSED\n",
    "\n",
    "Переходить к дальнейшим заданиям не имеем никакого смысла, пока вы не добьётесь прохождение теста\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU8G6jiQCgPx"
   },
   "source": [
    "#### Чтение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BF_svIwRCgPx",
    "outputId": "d3e55079-88eb-46cd-8fe6-8ab5840e017e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28) (60000, 10) (10000, 1, 28, 28) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)  # for reproducibility\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    " \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    " \n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZQodXAVCgPy"
   },
   "source": [
    "#### Подготовка моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-1bqPMGCgPy"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D\n",
    "\n",
    "def get_keras_model():\n",
    "    input_image = Input(shape=(1, 28, 28))\n",
    "    pool1 = MaxPooling2D(pool_size=(2,2), data_format='channels_first')(input_image)\n",
    "    flatten = Flatten()(pool1)\n",
    "    dense1 = Dense(10, activation='softmax')(flatten)\n",
    "    model = Model(inputs=input_image, outputs=dense1)\n",
    "\n",
    "    from keras.optimizers import Adam, SGD\n",
    "    sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, Y_train, validation_split=0.25, \n",
    "                        batch_size=32, epochs=2, verbose=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYlXoaPiCgPy"
   },
   "outputs": [],
   "source": [
    "def get_our_model(keras_model):\n",
    "    maxpool = MaxPooling()\n",
    "    flatten = FlattenLayer()\n",
    "    dense = DenseLayer(196, 10, W_init=keras_model.get_weights()[0],\n",
    "                       b_init=keras_model.get_weights()[1])\n",
    "    softmax = Softmax()\n",
    "    net = Network([maxpool, flatten, dense, softmax])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cr7YREzvCgPz",
    "outputId": "f87cd640-c2bc-44f8-aca5-b71b2c05223d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1407/1407 [==============================] - 7s 4ms/step - loss: 0.8329 - accuracy: 0.7758 - val_loss: 0.3785 - val_accuracy: 0.8927\n",
      "Epoch 2/2\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 0.3874 - accuracy: 0.8896 - val_loss: 0.3436 - val_accuracy: 0.9005\n"
     ]
    }
   ],
   "source": [
    "keras_model = get_keras_model()\n",
    "our_model = get_our_model(keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "891j5P5kCgPz"
   },
   "outputs": [],
   "source": [
    "keras_prediction = keras_model.predict(X_test)\n",
    "our_model_prediction = our_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VrzvY119CgPz",
    "outputId": "ddedc9eb-77da-4e12-9ed5-348d508156f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "if np.sum(np.abs(keras_prediction - our_model_prediction)) < 0.01:\n",
    "    print('Test PASSED')\n",
    "else:\n",
    "    print('Something went wrong!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrlFaf9dCgP0"
   },
   "source": [
    "### 2. Вычисление производных по входу для слоёв нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xM3_v07CgP0"
   },
   "source": [
    "#### 2.1 (1 балл) Реализуйте метод forward для класса CrossEntropy\n",
    "Напоминание: $$ crossentropy = L(p, y) =  - \\sum\\limits_i y_i log p_i, $$\n",
    "где вектор $(p_1, ..., p_k) $ -  выход классификационного алгоритма, а $(y_1,..., y_k)$ - правильные метки класса в унарной кодировке (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hn9PRPXWCgP0"
   },
   "outputs": [],
   "source": [
    "class CrossEntropy(object):\n",
    "    def __init__(self, eps=0.00001):\n",
    "        self.name = 'CrossEntropy'\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, input_data, labels):\n",
    "        return -np.sum(labels*np.log(input_data),axis = 1)\n",
    "    \n",
    "    def calculate_loss(self,input_data, labels):\n",
    "        return self.forward(input_data, labels)\n",
    "    \n",
    "    def grad_x(self, input_data, lables):\n",
    "        return -lables/input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nSUP-M9CgP0"
   },
   "source": [
    "#### 2.2 (2 баллa) Реализуйте метод grad_x класса CrossEntropy, который возвращает $\\frac{\\partial L}{\\partial p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQTxZ6sBCgP0"
   },
   "source": [
    "Проверить работоспособность кода поможет \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kdaC239mCgP1",
    "outputId": "843c6a56-9d8f-4aa0-cbf5-871312d65bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_diff_net(net, x, labels):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(len(x[0])):\n",
    "        delta = np.zeros(len(x[0]))\n",
    "        delta[i] = eps\n",
    "        diff = (net.calculate_loss(x + delta, labels) - net.calculate_loss(x-delta, labels)) / (2*eps)\n",
    "        right_answer.append(diff)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_net(net):\n",
    "    x = np.array([[1, 2, 3], [2, 3, 4]])\n",
    "    labels = np.array([[0.3, 0.2, 0.5], [0.3, 0.2, 0.5]])\n",
    "    num_grad = numerical_diff_net(net, x, labels)\n",
    "    grad = net.grad_x(x, labels)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your gradiend is ')\n",
    "        print(grad)\n",
    "        \n",
    "loss = CrossEntropy()\n",
    "test_net(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbl6FTVSCgP1"
   },
   "source": [
    "#### 2.3 (2 балла)   Реализуйте метод grad_x класса Softmax, который возвращает $\\frac{\\partial Softmax}{\\partial x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDpFUhmKCgP1"
   },
   "source": [
    "Проверить работоспособность кода поможет следующий тест:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XsC6xikgCgP1",
    "outputId": "f8393270-7f6f-4d70-c961-a721bb7e8dd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_diff_layer(layer, x):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(len(x[0])):\n",
    "        delta = np.zeros(len(x[0]))\n",
    "        delta[i] = eps\n",
    "        diff = (layer.forward(x + delta) - layer.forward(x-delta)) / (2*eps)\n",
    "        right_answer.append(diff.T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_layer(layer):\n",
    "    x = np.array([[1, 2, 3], [2, -3, 4]])\n",
    "    num_grad = numerical_diff_layer(layer, x)\n",
    "    grad = layer.grad_x(x)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your gradiend is ')\n",
    "        print(grad)\n",
    "        \n",
    "layer = Softmax()\n",
    "test_layer(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dAIqQmZCgP1"
   },
   "source": [
    "#### 2.4 (5 баллов) Реализуйте метод grad_x для классов ReLU и DenseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P41J4pc3CgP2",
    "outputId": "1163f2cb-e1d0-4c88-a297-6f0d18969d5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "layer = ReLU()\n",
    "test_layer(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "idICx5rrCgP2",
    "outputId": "79f044be-d4a1-4616-e35e-ccd969ab53cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "layer = DenseLayer(3,4)\n",
    "test_layer(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-g-O2rMyCgP2"
   },
   "source": [
    "#### 2.5 (4 балла) Для класса Network реализуйте метод grad_x, который должен реализовывать взятие производной от лосса по входу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RRx0l4DkCgP2",
    "outputId": "4631d423-4d48-4185-be7b-709c2d9fa792"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "net = Network([DenseLayer(3, 10), ReLU(), DenseLayer(10, 3), Softmax()], loss=CrossEntropy())\n",
    "test_net(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9-5CzF-CgP2"
   },
   "source": [
    "### 3. Реализация градиентов по параметрам и метода обратного распространения ошибки с обновлением парметров сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAP2gatSCgP2"
   },
   "source": [
    "#### 3.1 (4 балла) Реализуйте функции grad_b и grad_W. При подготовке теста grad_W предполагается, что W является одномерным вектором."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52hamZiGCgP3",
    "outputId": "5d4168b5-2427-4fa1-d975-3363c1f75656"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_grad_b(input_size, output_size, b, W, x):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(len(b)):\n",
    "        delta = np.zeros(b.shape)\n",
    "        delta[i] = eps\n",
    "        dense1 = DenseLayer(input_size, output_size, W_init=W, b_init=b+delta)\n",
    "        dense2 = DenseLayer(input_size, output_size, W_init=W, b_init=b-delta)\n",
    "        diff = (dense1.forward(x) - dense2.forward(x)) / (2*eps)\n",
    "        right_answer.append(diff.T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_grad_b():\n",
    "    input_size = 3\n",
    "    output_size = 4 \n",
    "    W_init = np.random.random((input_size, output_size))\n",
    "    b_init = np.random.random((output_size,))\n",
    "    x = np.random.random((2, input_size))\n",
    "    \n",
    "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
    "    grad = dense.grad_b(x)\n",
    "\n",
    "    num_grad = numerical_grad_b(input_size, output_size, b_init, W_init, x)\n",
    "\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your gradiend is ')\n",
    "        print(grad)\n",
    "\n",
    "test_grad_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8azvSajICgP3",
    "outputId": "dc0dcbe5-d03e-47f1-8945-11e1f0c7db0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_grad_W(input_size, output_size, b, W, x):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            delta = np.zeros(W.shape)\n",
    "            delta[i, j] = eps\n",
    "            dense1 = DenseLayer(input_size, output_size, W_init=W+delta, b_init=b)\n",
    "            dense2 = DenseLayer(input_size, output_size, W_init=W-delta, b_init=b)\n",
    "            diff = (dense1.forward(x) - dense2.forward(x)) / (2*eps)\n",
    "            right_answer.append(diff.T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_grad_W():\n",
    "    input_size = 3\n",
    "    output_size = 4 \n",
    "    W_init = np.random.random((input_size, output_size))\n",
    "    b_init = np.random.random((4,))\n",
    "    x = np.random.random((2, input_size))\n",
    "        \n",
    "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
    "    grad = dense.grad_W(x)\n",
    "\n",
    "    num_grad = numerical_grad_W(input_size, output_size, b_init, W_init, x)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your gradiend is ')\n",
    "        print(grad)\n",
    "\n",
    "test_grad_W()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1vMVv02CgP3"
   },
   "source": [
    "#### 3.2 (4 балла) Полностью реализуйте метод обратного распространения ошибки в функции train_step класса Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTOoFdwFCgP3"
   },
   "source": [
    "Рекомендуем реализовать сначала функцию Network.grad_param(), которая возвращает список длиной в количество слоёв и элементом которого является список градиентов по параметрам.\n",
    "После чего, имея список градиентов, написать функцию обновления параметров для каждого слоя. \n",
    "\n",
    "Совет: рекомендуем написать тест для кода подсчета градиента по параметрам, чтобы быть уверенным в том, что градиент через всю сеть считается правильно\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-epReQACgP3"
   },
   "source": [
    "#### 3.3 Ознакомьтесь с реализацией функции fit класса Network. Запустите обучение модели. Если всё работает правильно, то точность на валидации должна будет возрастать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5C5lHOTCgP3",
    "outputId": "eb22bb44-0832-4885-d0fe-f3a9ae19971f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [00:05<00:00, 171.85it/s]\n",
      "  2%|▏         | 19/937 [00:00<00:05, 179.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch: val 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [00:05<00:00, 170.50it/s]\n",
      "  2%|▏         | 18/937 [00:00<00:05, 172.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 epoch: val 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [00:05<00:00, 170.82it/s]\n",
      "  2%|▏         | 19/937 [00:00<00:04, 184.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 epoch: val 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [00:05<00:00, 170.91it/s]\n",
      "  2%|▏         | 16/937 [00:00<00:06, 151.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 epoch: val 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [00:05<00:00, 171.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epoch: val 0.85\n"
     ]
    }
   ],
   "source": [
    "net = Network([DenseLayer(784, 10), Softmax()], loss=CrossEntropy())\n",
    "trainX = X_train.reshape(len(X_train), -1)\n",
    "net.fit(trainX[::3], Y_train[::3], validation_split=0.25, \n",
    "            batch_size=16, nb_epoch=5, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UjKU1mR5CgP4",
    "outputId": "50a8fe28-c7e6-4f0a-8fbe-f0f9ead96eea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:07<00:00, 59.79it/s]\n",
      "  1%|▏         | 7/468 [00:00<00:07, 63.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch: val 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:07<00:00, 61.42it/s]\n",
      "  1%|▏         | 6/468 [00:00<00:08, 52.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 epoch: val 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:08<00:00, 55.16it/s]\n",
      "  1%|▏         | 7/468 [00:00<00:06, 67.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 epoch: val 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:07<00:00, 65.35it/s]\n",
      "  1%|▏         | 7/468 [00:00<00:06, 66.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 epoch: val 0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:07<00:00, 60.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epoch: val 0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = Network([DenseLayer(784, 20), ReLU(), DenseLayer(20, 10), Softmax()], loss=CrossEntropy())\n",
    "trainX = X_train.reshape(len(X_train), -1)\n",
    "net.fit(trainX[::6], Y_train[::6], validation_split=0.25, \n",
    "            batch_size=16, nb_epoch=5, learning_rate=0.001)    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Prac02_Mironenko.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
